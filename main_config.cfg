[args]
# Bert pre-trained model selected in the list [bert-base-cased, roberta-base, albert-base-v1 / albert-large-v1] (default = roberta-base)
bert_model = roberta-base

# which dataset used (vua18 / vua20 / trofi / mohx / sample)
dataset_name = sample
# The name of model type (default = MELBERT) (BERT_BASE / BERT_SEQ / MELBERT_SPV / MELBERT_MIP / MELBERT)
model_type = MELBERT_GAT



# The hidden dimension for classifier (default = 768)
classifier_hidden = 768
# Learning rate scheduler (default = warmup_linear) (none / warmup_linear)
lr_schedule = warmup_linear
# Training epochs to perform linear learning rate warmup for. (default = 2)
warmup_epoch = 2
# Dropout ratio (default = 0.2)
drop_ratio = 0.2
# K-fold (default = 10)
kfold = 10
# Number of bagging (default = 0) (0 not for using bagging technique)
num_bagging = 0
# The index of bagging only for the case using bagging technique (default = 0)
bagging_index = 0
# The name of the task to train
task_name = vua18

# Use additional linguistic features
# POS tag (default = True)
use_pos = True
# Local context (default = True)
use_local_context= True

# The maximum total input sequence length after WordPiece tokenization. (default = 200)
max_seq_length = 500
# Whether to run training (default = False)
do_train = True
# Whether to run eval on the test set (default = False)
do_test = True
# Whether to run eval on the dev set. (default = False)
do_eval = True
# Set this flag if you are using an uncased model. (default = False)
do_lower_case = False
# Weight of metaphor. (default = 3.0)
class_weight = 3
# Total batch size for training. (default = 32)
train_batch_size = 16
# Total batch size for eval. (default = 8)
eval_batch_size = 4
# The initial learning rate for Adam (default = 3e-5)
learning_rate = 3e-5
# Total number of training epochs to perform. (default = 3.0)
num_train_epoch = 3

# Whether not to use CUDA when available (default = False)
no_cuda = False
# random seed for initialization (default = 42)
seed = 42

# GAT part
embedding_dim = 768
num_heads = 6
dep_tag_num = 49
pos_tag_num = 19
data_dir = data_sample/VUA

# GAT data
num_classes = 2

melbert = True
pure_bert = False
pure_roberta = False
roberta_frame = False
concept_rgat = False
frame_finder = False
gat_bert = False
gat = False
gat_our = False

multi_hop = True
max_hop = 4
add_non_connect = True
embedding_type = melbert

per_gpu_train_batch_size = 16
per_gpu_eval_batch_size = 4

